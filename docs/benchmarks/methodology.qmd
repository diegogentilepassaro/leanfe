---
title: "Benchmark Methodology"
subtitle: "Transparent and reproducible performance testing"
---

## Overview

This page documents our benchmark methodology to ensure transparency and reproducibility. All benchmarks can be reproduced using the provided scripts.

## YOCO Compression + Sparse Matrices

leanfe uses the **YOCO (You Only Compress Once)** strategy from [Wong et al. (2021)](https://arxiv.org/abs/2102.11297) combined with **sparse matrices** for **all standard error types** (IID, HC1, and clustered):

### For IID/HC1 Standard Errors

1. **Compress**: GROUP BY (regressors + fixed effects) → compute `n`, `sum(y)`, `sum(y²)` per group
2. **Build sparse design matrix**: FE dummies as `scipy.sparse` (Python) or `Matrix` (R)
3. **Solve sparse WLS**: Weighted least squares with sparse operations
4. **Standard errors**: Compute from grouped residual sum of squares

### For Clustered Standard Errors (Section 5.3.1)

Clustered SEs require additional steps to aggregate scores within clusters:

1. **Within-cluster compression**: GROUP BY (regressors + fixed effects + **cluster_id**) — each compressed record belongs to exactly one cluster
2. **Build sparse design matrix**: Same as above
3. **Solve sparse WLS**: Same as above
4. **Compute ẽ⁰**: Sum of residuals per group = `sum_y - n × ŷ`
5. **Build sparse cluster matrix W̃_C**: Indicator matrix (G × C) mapping groups to clusters
6. **Aggregate scores**: `cluster_scores = W̃_Cᵀ @ (X × ẽ⁰)` — vectorized, no loops!
7. **Meat matrix**: `meat = cluster_scoresᵀ @ cluster_scores`
8. **Sandwich**: `V(β̂) = (X'X)⁻¹ × meat × (X'X)⁻¹ × adjustment`

The sparse cluster matrix W̃_C enables **vectorized aggregation** across all clusters simultaneously, avoiding the traditional loop over C clusters.

This is **lossless** - coefficients and standard errors are identical to the full computation, but up to **23x faster** for discrete regressors at scale.

### Why Sparse Matrices?

With many FE levels, the design matrix has many columns but only a few non-zeros per row (one per FE). Sparse matrices:

- **Memory**: Store only non-zero entries
- **Speed**: Skip zero multiplications in matrix operations
- **Scalability**: Time stays nearly constant regardless of FE cardinality

For clustered SEs, the cluster matrix W̃_C is also sparse (one 1 per row), enabling efficient score aggregation even with thousands of clusters.

## Data Generation

### Synthetic Panel Data

All benchmarks use the same standardized data generation for consistency:

```python
import numpy as np
import polars as pl

def generate_benchmark_data(n_obs, n_fe1=500, n_fe2=100, n_clusters=None, seed=42):
    """
    Generate synthetic benchmark data.
    
    Parameters
    ----------
    n_obs : int
        Number of observations
    n_fe1 : int
        Number of FE1 levels (default 500, e.g., firms)
    n_fe2 : int
        Number of FE2 levels (default 100, e.g., products)
    n_clusters : int, optional
        Number of clusters for clustered SE benchmarks
    seed : int
        Random seed for reproducibility
    """
    np.random.seed(seed)
    
    # Fixed effect IDs
    fe1 = np.random.randint(1, n_fe1 + 1, n_obs)
    fe2 = np.random.randint(1, n_fe2 + 1, n_obs)
    
    # Fixed effect values
    fe1_effects = np.random.normal(0, 1, n_fe1 + 1)[fe1]
    fe2_effects = np.random.normal(0, 0.5, n_fe2 + 1)[fe2]
    
    # Regressors - discrete for YOCO compression benefits
    treatment = np.random.binomial(1, 0.3, n_obs).astype(float)
    x1 = np.random.choice([0.0, 1.0, 2.0], n_obs)  # Discrete
    
    # Outcome: y = 2.5*treatment + 1.5*x1 + fe1 + fe2 + noise
    y = (2.5 * treatment + 1.5 * x1 + 
         fe1_effects + fe2_effects + 
         np.random.normal(0, 1, n_obs))
    
    data = {
        "y": y,
        "treatment": treatment,
        "x1": x1,
        "fe1": fe1,
        "fe2": fe2,
    }
    
    # Add cluster column if needed
    if n_clusters is not None:
        cluster_id = np.random.randint(0, n_clusters, n_obs)
        cluster_effects = np.random.normal(0, 0.5, n_clusters)[cluster_id]
        data["cluster_id"] = cluster_id
        data["y"] = y + cluster_effects  # Add cluster-level variation
    
    return pl.DataFrame(data)
```

### Data Characteristics

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| True treatment effect | 2.5 | Known ground truth for validation |
| True x1 effect | 1.5 | Known ground truth for validation |
| FE1 cardinality | 500 | High-dimensional (e.g., firms) |
| FE2 cardinality | 100 | Medium-dimensional (e.g., products) |
| Treatment | Binary (0/1) | 30% probability, discrete for compression |
| x1 | Discrete (0, 1, 2) | Enables YOCO compression |
| Error variance | 1.0 | Standard normal errors |
| Seed | 42 | Reproducibility |

### Dataset Sizes

All benchmarks use consistent sizes for comparability:

| Size | Observations | Use Case |
|------|--------------|----------|
| Small | 100,000 | Quick tests |
| Medium | 1,000,000 | Standard benchmarks |
| Large | 10,000,000 | Stress tests |
| XL | 100,000,000 | Memory efficiency / scale tests |

## Hardware and Software

### Test Environment

Benchmarks were run on:

- **Machine**: Apple MacBook Pro
- **Processor**: Apple M-series
- **RAM**: 16 GB
- **OS**: macOS

### Software Versions

```{python}
#| echo: true
#| output: true

import sys
import polars
import duckdb
import numpy

print("Software Versions:")
print(f"  Python: {sys.version.split()[0]}")
print(f"  Polars: {polars.__version__}")
print(f"  DuckDB: {duckdb.__version__}")
print(f"  NumPy: {numpy.__version__}")
```

## Measurement Methodology

### Time Measurement

```python
import time

start = time.time()
result = leanfe(...)
elapsed = time.time() - start
```

- **Metric**: Wall-clock time in seconds
- **Iterations**: 3 runs per configuration
- **Reported**: Mean time across iterations

### Memory Measurement

```python
import tracemalloc

tracemalloc.start()
result = leanfe(...)
current, peak = tracemalloc.get_traced_memory()
tracemalloc.stop()
```

- **Metric**: Peak memory usage in MB
- **Note**: Measures Python heap only, not total process memory

## Validation

### Coefficient Matching

All benchmarks verify that packages produce matching coefficients:

```python
tolerance = 1e-6

polars_coef = result_polars['coefficients']['treatment']
duckdb_coef = result_duckdb['coefficients']['treatment']

assert abs(polars_coef - duckdb_coef) < tolerance
```

### Ground Truth Comparison

With known true coefficients (treatment=2.5, x1=1.5), we verify estimates are close:

```{python}
#| echo: true
#| output: true

import numpy as np
import polars as pl
from leanfe import leanfe

# Generate data with known coefficients
np.random.seed(42)
n = 100_000

fe1 = np.random.randint(1, 501, n)
fe2 = np.random.randint(1, 101, n)
fe1_effects = np.random.normal(0, 1, 501)[fe1]
fe2_effects = np.random.normal(0, 0.5, 101)[fe2]

treatment = np.random.binomial(1, 0.3, n).astype(float)
x1 = np.random.normal(0, 1, n)

# True coefficients: treatment=2.5, x1=1.5
y = 2.5 * treatment + 1.5 * x1 + fe1_effects + fe2_effects + np.random.normal(0, 1, n)

df = pl.DataFrame({"y": y, "treatment": treatment, "x1": x1, "fe1": fe1, "fe2": fe2})

result = leanfe(formula="y ~ treatment + x1 | fe1 + fe2", data=df, vcov="iid")

print("Ground Truth Validation:")
print(f"  True treatment: 2.5, Estimated: {result['coefficients']['treatment']:.4f}")
print(f"  True x1: 1.5, Estimated: {result['coefficients']['x1']:.4f}")
```

## Limitations and Caveats

### What These Benchmarks Show

✅ Relative performance between leanfe backends
✅ Scaling behavior with dataset size
✅ Memory efficiency of DuckDB backend

### What These Benchmarks Don't Show

❌ Comparison with PyFixest/fixest (different feature sets)
❌ Performance on real-world data (may vary)
❌ GPU acceleration (not implemented)

### Known Limitations

1. **Synthetic data**: Real data may have different characteristics
2. **Single machine**: Results may vary on different hardware
3. **Python overhead**: Timing includes Python interpreter overhead
4. **Memory measurement**: tracemalloc may underestimate total memory

## Reproducible Scripts

### Download Benchmark Code

The complete benchmark suite is available in the repository:

```bash
# Clone repository
git clone https://github.com/diegogentilepassaro/leanfe.git

# Navigate to benchmarks
cd leanfe/package/docs/benchmarks

# Run benchmarks
python benchmark_runner.py
```

### Quick Benchmark

```python
from leanfe import leanfe
import numpy as np
import polars as pl
import time

# Generate data
np.random.seed(42)
n = 1_000_000
df = pl.DataFrame({
    "y": np.random.normal(0, 1, n),
    "x": np.random.normal(0, 1, n),
    "fe1": np.random.randint(1, 1001, n),
    "fe2": np.random.randint(1, 501, n),
})

# Benchmark Polars
start = time.time()
leanfe(data=df, formula="y ~ x | fe1 + fe2", backend="polars")
print(f"Polars: {time.time() - start:.2f}s")

# Benchmark DuckDB
start = time.time()
leanfe(data=df, formula="y ~ x | fe1 + fe2", backend="duckdb")
print(f"DuckDB: {time.time() - start:.2f}s")
```

## See Also

- [Benchmark Results](overview.qmd)
- [Large Datasets Guide](../guides/large-datasets.qmd)
